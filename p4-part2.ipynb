{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA Project\n",
    "\n",
    "Now that we understand the dataset, we are going to try to answer our research questions in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataframes\n",
    "all_games = pd.read_pickle(\"data/games.pkl\")\n",
    "all_orders = pd.read_pickle(\"data/orders.pkl\")\n",
    "all_players = pd.read_pickle(\"data/players.pkl\")\n",
    "all_turns = pd.read_pickle(\"data/turns.pkl\")\n",
    "all_units = pd.read_pickle(\"data/units.pkl\")\n",
    "\n",
    "# remove duplicates\n",
    "all_units = all_units.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive statistics\n",
    "\n",
    "We look at the **outcome** (binary) considering the **treatment**, which can be:\n",
    "- engaged: player who was engaged in a friendship\n",
    "- single: player who was not engaged in a friendship\n",
    "\n",
    "And for engaged player, there is an aditional treatment: \n",
    "- betrayer: player who betrayed another one\n",
    "- betrayed: player who ended up betrayed by another\n",
    "- neutral: player who was not engaged in a *broken* friendship\n",
    "\n",
    "In order to answer this question, let's define a few functions that we will use in the later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = ['A', 'E', 'F', 'G', 'I', 'R', 'T']\n",
    "pairs = [x+y for x in countries for y in countries if y > x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_engageds(friendships):\n",
    "    \"\"\"Returns the players engaged in a friendship\"\"\"\n",
    "    cols = [col for col in friendships.columns if np.count_nonzero(friendships[col] != 0)]\n",
    "    return list(set(''.join(cols)))\n",
    "\n",
    "def get_singles(engageds):\n",
    "    \"\"\"Returns players not engaged in a friendship\"\"\"\n",
    "    singles = countries.copy()\n",
    "    for engaged in engageds:\n",
    "        singles.remove(engaged)\n",
    "    return singles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_betrayers_and_betrayed(friendships):\n",
    "    \"\"\"Given the Friendships dataframe as defined in our analysis, returns all the players who commited \n",
    "    betrayals and all players who ended up betrayed\"\"\"\n",
    "    cols = [col for col in friendships.columns if np.count_nonzero(friendships[col] != 0)]\n",
    "    betrayers = []\n",
    "    betrayeds = []\n",
    "    for c in cols: \n",
    "        tmp = friendships[c]\n",
    "        values = tmp[tmp != 0].values\n",
    "        if type(values[-1]) == str: \n",
    "            betrayer = values[-1]\n",
    "            betrayers.append(betrayer)\n",
    "            tmp = list(c)\n",
    "            tmp.remove(betrayer)\n",
    "            betrayeds.append(tmp[0])\n",
    "            \n",
    "    return betrayers, betrayeds\n",
    "\n",
    "def get_neutrals(betrayers, betrayeds):\n",
    "    \"\"\"Given betrayers and betrayeds players of a game, returns the list of players\n",
    "    who were not involved in a broken friendship\n",
    "    \n",
    "    Required: player must be 'engaged'\n",
    "    \"\"\"\n",
    "    neutrals = countries.copy()\n",
    "    for b in betrayers: \n",
    "        if b in neutrals: neutrals.remove(b)\n",
    "    for b in betrayeds: \n",
    "        if b in neutrals: neutrals.remove(b)\n",
    "    return neutrals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_winners(game_id):\n",
    "    winner = all_players.query(\"game_id == @game_id & won == 1\")\n",
    "    return winner.country.values\n",
    "\n",
    "def get_losers(winners):\n",
    "    loosers = countries.copy()\n",
    "    for w in winners: loosers.remove(w)\n",
    "    return loosers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data to analyse\n",
    "games_id = np.load(\"data/subset_T500_N1/games_id.npy\")\n",
    "all_friendships = np.load(\"data/subset_T500_N1/friendships.npy\", allow_pickle=True)\n",
    "verbose = False\n",
    "\n",
    "data_overall = np.zeros(shape = (2,2))\n",
    "data_engaged = np.zeros(shape = (3,2))\n",
    "\n",
    "treatments_overall = [\"single\", \"engaged\"]\n",
    "treatments_engaged = [\"betrayer\", \"betrayed\", \"neutral\"]\n",
    "outcomes = [\"winner\", \"loser\"]\n",
    "\n",
    "stats_overall = pd.DataFrame(data_overall, index = treatments_overall, columns = outcomes )\n",
    "stats_engageds = pd.DataFrame(data_engaged, index = treatments_engaged, columns = outcomes )\n",
    "\n",
    "N = len(games_id)\n",
    "for i, game_id in enumerate(games_id):\n",
    "    # reconstruct the obtained data\n",
    "    data = all_friendships[i]\n",
    "    years = np.arange(1901, 1901 + data.shape[0] * 0.5, 0.5)\n",
    "    friendships = pd.DataFrame(data = all_friendships[i], columns = pairs, index = years)\n",
    "    \n",
    "    # get outcomes\n",
    "    winners = get_winners(game_id)\n",
    "    losers = get_losers(winners)\n",
    "    \n",
    "    # get treatment 1 \n",
    "    engageds = get_engageds(friendships)\n",
    "    singles = get_singles(engageds)\n",
    "    \n",
    "    # get treatment 2\n",
    "    betrayers, betrayeds = get_betrayers_and_betrayed(friendships)\n",
    "    neutrals = get_neutrals(betrayers, betrayeds)\n",
    "    \n",
    "    # overal statistics\n",
    "    for w in winners:\n",
    "        if w in singles: stats_overall.loc[\"single\", \"winner\"] += 1\n",
    "        if w in engageds: stats_overall.loc[\"engaged\", \"winner\"] += 1\n",
    "    for l in losers:\n",
    "        if l in singles: stats_overall.loc[\"single\", \"loser\"] += 1\n",
    "        if l in engageds: stats_overall.loc[\"engaged\", \"loser\"] += 1\n",
    "    \n",
    "    # statistics about betrayers: \n",
    "    for winner in winners: \n",
    "        if winner in engageds: \n",
    "            if winner in betrayers: stats_engageds.loc[\"betrayer\", \"winner\"] += 1\n",
    "            if winner in betrayeds: stats_engageds.loc[\"betrayed\", \"winner\"] += 1\n",
    "            if winner in neutrals: stats_engageds.loc[\"neutral\", \"winner\"] += 1\n",
    "    for loser in losers: \n",
    "        if winner in engageds:\n",
    "            if loser in betrayers: stats_engageds.loc[\"betrayer\", \"loser\"] += 1\n",
    "            if loser in betrayeds: stats_engageds.loc[\"betrayed\", \"loser\"] += 1\n",
    "            if loser in neutrals: stats_engageds.loc[\"neutral\", \"loser\"] += 1\n",
    "            \n",
    "    if verbose:\n",
    "        print(\"\\nGame\",i)\n",
    "        print(\"Winners: \", winners, \" and Losers\", losers)\n",
    "        print(\"Betrayers: \", betrayers, \" and Betrayed\", betrayeds)\n",
    "        print(\"Neutrals: \", neutrals)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics over all dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>winner</th>\n",
       "      <th>loser</th>\n",
       "      <th>win_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>single</th>\n",
       "      <td>186.0</td>\n",
       "      <td>1947.0</td>\n",
       "      <td>0.087201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>engaged</th>\n",
       "      <td>371.0</td>\n",
       "      <td>996.0</td>\n",
       "      <td>0.271397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         winner   loser  win_ratio\n",
       "single    186.0  1947.0   0.087201\n",
       "engaged   371.0   996.0   0.271397"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "win_ratio = stats_overall.winner / (stats_overall.loser + stats_overall.winner)\n",
    "stats_overall[\"win_ratio\"] = win_ratio\n",
    "print(\"Statistics over all dataset\")\n",
    "stats_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics over players engaged in a friendship\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>winner</th>\n",
       "      <th>loser</th>\n",
       "      <th>win_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>betrayer</th>\n",
       "      <td>133.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.513514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>betrayed</th>\n",
       "      <td>70.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.270270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>171.0</td>\n",
       "      <td>1610.0</td>\n",
       "      <td>0.096013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          winner   loser  win_ratio\n",
       "betrayer   133.0   126.0   0.513514\n",
       "betrayed    70.0   189.0   0.270270\n",
       "neutral    171.0  1610.0   0.096013"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "win_ratio = stats_engageds.winner / (stats_engageds.loser + stats_engageds.winner)\n",
    "stats_engageds[\"win_ratio\"] = win_ratio\n",
    "print(\"Statistics over players engaged in a friendship\")\n",
    "stats_engageds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we see here ? \n",
    "- among all players that were involved in a broken friendship (either *betrayer* or *betrayed*) the chances of winning go towards the betrayer. Betrayed players have much higher chances of loosing and about 5 times less chances of winning. . .. \n",
    "- the neutral players represents the majority of players, however their chances of winning are not much bigger than the chances of win than betrayed people. \n",
    "\n",
    "These results make us strongly believe that **betrayals strongly influence the outcome of the game**. \n",
    "\n",
    "What can we do next ? \n",
    "\n",
    "- select 250 games with  and without betrayals (and all with friendships), then do a matching based on the games properties, and look at what differs once a betrayal happened for the players who were engaged in a friendship !\n",
    "\n",
    "- quantify agressivity of players towards others, and using the same dataset as before, try to see what happens to a player that was betrayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Analysis: can we predict betrayals using game features ? \n",
    "\n",
    "We want to perform a logistic regression with our collected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game_id</th>\n",
       "      <th>has_betrayal</th>\n",
       "      <th>length</th>\n",
       "      <th>outcome</th>\n",
       "      <th>n_aoh_x</th>\n",
       "      <th>n_aoh_y</th>\n",
       "      <th>n_aos_x</th>\n",
       "      <th>n_aos_y</th>\n",
       "      <th>supports_xy</th>\n",
       "      <th>supports_yx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70538.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70538.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   game_id has_betrayal  length  outcome  n_aoh_x  n_aoh_y  n_aos_x  n_aos_y  \\\n",
       "0  70538.0         True     1.0      0.0      5.0      2.0      0.0      0.0   \n",
       "1  70538.0         True     2.0      0.0      2.0      2.0      0.0      0.0   \n",
       "\n",
       "   supports_xy  supports_yx  \n",
       "0          0.0          1.0  \n",
       "1          1.0          1.0  "
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "features = pd.read_pickle(\"data/features_B200_V0_N1/features.pkl\")\n",
    "features.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Normalisation \n",
    "predictors = [\"length\", \"n_aoh_x\", \"n_aoh_y\",\"n_aos_x\", \"n_aos_y\", \"supports_xy\", \"supports_yx\"]\n",
    "features = features[predictors + [\"outcome\"]]\n",
    "\n",
    "def normalise_preds(features, predictors):\n",
    "    for p in predictors: \n",
    "        features[p] = (features[p] - features[p].mean()) / features[p].std()\n",
    "        \n",
    "normalise_preds(features, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'outcome ~ length + n_aoh_x + n_aoh_y + n_aos_x + n_aos_y + supports_xy + supports_yx'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Create the regression formula\n",
    "formula = \"outcome ~ \" + ' + '.join(predictors)\n",
    "formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.250384\n",
      "         Iterations 7\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                outcome   No. Observations:                 3546\n",
      "Model:                          Logit   Df Residuals:                     3538\n",
      "Method:                           MLE   Df Model:                            7\n",
      "Date:                Sat, 12 Dec 2020   Pseudo R-squ.:                 0.04747\n",
      "Time:                        08:44:58   Log-Likelihood:                -887.86\n",
      "converged:                       True   LL-Null:                       -932.11\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.510e-16\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "Intercept      -2.6813      0.072    -37.001      0.000      -2.823      -2.539\n",
      "length          0.3673      0.056      6.530      0.000       0.257       0.477\n",
      "n_aoh_x        -0.0304      0.065     -0.464      0.642      -0.159       0.098\n",
      "n_aoh_y         0.1605      0.062      2.575      0.010       0.038       0.283\n",
      "n_aos_x         0.0511      0.062      0.828      0.408      -0.070       0.172\n",
      "n_aos_y        -0.0352      0.072     -0.490      0.624      -0.176       0.106\n",
      "supports_xy     0.4084      0.064      6.380      0.000       0.283       0.534\n",
      "supports_yx     0.1786      0.064      2.774      0.006       0.052       0.305\n",
      "===============================================================================\n"
     ]
    }
   ],
   "source": [
    "# 3. Make the regression\n",
    "mod = smf.logit(formula = formula, data = features)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Assessment\n",
    "\n",
    "Now that we have fitted a regression model, it is quite hard to interpret the summary. However, we can use additional metrics to see if our model is doing anything like we want to it.\n",
    "\n",
    "It was for this purpose that we generated another set of points: the testing dataset.\n",
    "\n",
    "Let's also focus our attention on the seasons where a real betrayal happened, and let's see if this classifier worked properly. It is for this reason that we test the results on the seasons of the testing set with betrayal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
       "       1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. read the test dataset\n",
    "features_test = pd.read_pickle(\"data/features_B20_V0_N1/features.pkl\")\n",
    "\n",
    "# normalise_preds(features_test, predictors)\n",
    "\n",
    "X_test = features_test.query(\"outcome == 1\")[predictors]\n",
    "y_test = features_test.query(\"outcome == 1\")[\"outcome\"]\n",
    "\n",
    "# 2. predict values and get the binary output\n",
    "predictions = res.predict(X_test)\n",
    "binaries = predictions.values.round()\n",
    "binaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first metric we can look at is the [accuracy score](https://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4482758620689655"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. accuracy score\n",
    "accuracy_score(y_test, binaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we obtain about 45% of accuracy. Is this good ? Well, it is possible to compare this result with a **dummy estimator** to see if our models performs better than stupid ones. Here, there are several possibilities.\n",
    "- The 'dumbest' estimator will perform better than this one. Indeed, guessing uniformly between either zero or one will give a 50% chance level to detect betrayals, when only trying to detect betrayals. So when looking dumbly at this task, it looks like what we have isn't good. However, such a result isn't so useful: indeed, there is a strong imbalance in the class labels and if a classifier says that only 50% of seasons are betrayal, it's a really bad. The reason it performs better here is because we selected only seasons where a betrayal happened.\n",
    "- Another 'dummy' estimator is a stratified one: it generates random predictions by respecting the training set class distribution. We using this one, we obtain a classification score of 10%, which is now a lot less than what was obtained with our predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10344827586206896"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. dummy estimator for sanity check\n",
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier(strategy='stratified', random_state=0)\n",
    "X_train = features[predictors]\n",
    "y_train = features[\"outcome\"]\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to understand more using the confusion matrix. Here, betrayals are marked as negatives. The rows represent the actual classes (first row means no betrayal, second row means betrayal) and the columns represent the predicted class (first row means predicted as no betrayals, second row means predicted as betrayal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0],\n",
       "       [16, 13]])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import (confusion_matrix, accuracy_score, plot_confusion_matrix) \n",
    "m = confusion_matrix(y_test, binaries)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our predictor, about 45% of betrayals are detected as so. One could think that this information is not enough for proclaming \"good classifier\", because what matters isn't only the 'True Betrayer' but also what happens to the seasons leading to a betrayer (i.e., what about 'False Betrayal' and 'True No Betrayal'. To investigate this result, one can run our previous analysis again without looking only at seasons where betrayal happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.671578947368421"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = features_test[predictors]\n",
    "y_test = features_test[\"outcome\"]\n",
    "\n",
    "# 2. predict values and get the binary output\n",
    "predictions = res.predict(X_test)\n",
    "binaries = predictions.values.round()\n",
    "accuracy_score(y_test, binaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8652631578947368"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DummyClassifier(strategy='stratified', random_state=0)\n",
    "X_train = features[predictors]\n",
    "y_train = features[\"outcome\"]\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[306, 140],\n",
       "       [ 16,  13]])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = confusion_matrix(y_test, binaries)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we observe ? The predictor has an overall accuracy of 67%: **this is better than random guess, however it is less than a most-frequent choice** (which guess based on the class distribution, and hence is garuanted to have a much higher accuracy since there is a strong class imbalance). Again, because of the **strong class imbalance**, the accuracy isn't enough to determine, but we can look at the confusion matrix. Again, for this matrix, 'Betrayer' is marked as 'Negatif'.\n",
    "- The second column of $C$ is same as before, revealing that accuracy of detecting betrayer is about 45%. \n",
    "- The first column shows that $306 / 446 \\approx 0.69 \\%$ of non betrayals seasons are classifed as so, meaning that there are actually quite some seasons that are detected as betrayal when they are not. \n",
    "\n",
    "**TODO: talk about the results of the paper here!**\n",
    "\n",
    "Like in what the paper did, we can ask ourself this question: **is it the case because some betrayals are detected too early ?**. Another question that we ask is **is it possible that more than one betrayals are detected within each game, and if so, how can we enforce that there's only one betrayal per game ?**\n",
    "\n",
    "The first thing to do is to try to look at the results per games, using our training set. Then we can look at one game and see what's hapenning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game_id</th>\n",
       "      <th>has_betrayal</th>\n",
       "      <th>length</th>\n",
       "      <th>outcome</th>\n",
       "      <th>n_aoh_x</th>\n",
       "      <th>n_aoh_y</th>\n",
       "      <th>n_aos_x</th>\n",
       "      <th>n_aos_y</th>\n",
       "      <th>supports_xy</th>\n",
       "      <th>supports_yx</th>\n",
       "      <th>outcome_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52531.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52531.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52531.0</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52531.0</td>\n",
       "      <td>False</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52531.0</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>52531.0</td>\n",
       "      <td>False</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>52531.0</td>\n",
       "      <td>False</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>52531.0</td>\n",
       "      <td>False</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>52531.0</td>\n",
       "      <td>False</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>52531.0</td>\n",
       "      <td>False</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>52531.0</td>\n",
       "      <td>False</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52531.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52531.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52531.0</td>\n",
       "      <td>True</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52531.0</td>\n",
       "      <td>True</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    game_id has_betrayal  length  outcome  n_aoh_x  n_aoh_y  n_aos_x  n_aos_y  \\\n",
       "0   52531.0        False     1.0      0.0      2.0      0.0      0.0      0.0   \n",
       "1   52531.0        False     2.0      0.0      2.0      2.0      0.0      0.0   \n",
       "2   52531.0        False     3.0      0.0      1.0      1.0      0.0      2.0   \n",
       "3   52531.0        False     4.0      0.0      1.0      2.0      0.0      1.0   \n",
       "4   52531.0        False     5.0      0.0      1.0      1.0      2.0      0.0   \n",
       "5   52531.0        False     6.0      0.0      1.0      0.0      0.0      0.0   \n",
       "6   52531.0        False     7.0      0.0      0.0      2.0      0.0      2.0   \n",
       "7   52531.0        False     8.0      0.0      0.0      3.0      0.0      1.0   \n",
       "8   52531.0        False     9.0      0.0      4.0      0.0      0.0      0.0   \n",
       "9   52531.0        False    10.0      0.0      0.0      3.0      0.0      0.0   \n",
       "10  52531.0        False    11.0      0.0      1.0      2.0      0.0      0.0   \n",
       "0   52531.0         True     1.0      0.0      2.0      1.0      1.0      1.0   \n",
       "1   52531.0         True     2.0      0.0      1.0      3.0      0.0      1.0   \n",
       "2   52531.0         True     3.0      0.0      2.0      0.0      0.0      0.0   \n",
       "3   52531.0         True     4.0      1.0      1.0      1.0      0.0      0.0   \n",
       "\n",
       "    supports_xy  supports_yx  outcome_pred  \n",
       "0      1.000000          0.0           0.0  \n",
       "1      0.000000          1.0           0.0  \n",
       "2      0.000000          1.0           0.0  \n",
       "3      0.000000          1.0           0.0  \n",
       "4      0.000000          1.0           0.0  \n",
       "5      0.000000          1.5           0.0  \n",
       "6      1.500000          0.0           1.0  \n",
       "7      2.000000          1.0           1.0  \n",
       "8      1.000000          0.0           1.0  \n",
       "9      0.000000          1.0           1.0  \n",
       "10     1.333333          0.0           1.0  \n",
       "0      0.000000          1.0           0.0  \n",
       "1      2.000000          1.0           0.0  \n",
       "2      1.500000          1.0           0.0  \n",
       "3      1.666667          0.0           0.0  "
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add the predicted outcome\n",
    "features_test[\"outcome_pred\"] = binaries\n",
    "features_test.query(\"game_id == 52531.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
